# -*- coding: utf-8 -*-
"""problem3.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jtPzgnY6UW7P1wQA6i3lW4ew6aWGO_bP
"""

!pip install torch torchvision matplotlib numpy

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
import os # For saving the model

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Generator Model
class Generator(nn.Module):
    def __init__(self, z_dim=100, img_dim=784):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(z_dim + 10, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        x = torch.cat([noise, labels], dim=1)
        return self.model(x)

# Label embedding function
def label_to_onehot(labels, num_classes=10):
    return torch.eye(num_classes)[labels]

# Parameters
z_dim = 100
lr = 0.0002
batch_size = 128
epochs = 5

# Data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_data = datasets.MNIST(root='.', train=True, transform=transform, download=True)
train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)

# Model, Loss, Optimizer
G = Generator().to(device)
criterion = nn.BCELoss()
optimizer = optim.Adam(G.parameters(), lr=lr)

# Training loop
for epoch in range(epochs):
    for batch_idx, (real, labels) in enumerate(train_loader):
        batch_size = real.size(0)
        real = real.view(batch_size, -1).to(device)
        labels = label_to_onehot(labels).to(device)
        noise = torch.randn(batch_size, z_dim).to(device)

        fake = G(noise, labels)
        loss = ((fake - real) ** 2).mean()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch+1}/{epochs}] Loss: {loss.item():.4f}")

# Save the generator
torch.save(G.state_dict(), "mnist_generator.pth")

!pip install streamlit

# [app.py] - Streamlit Web App
import streamlit as st
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

class Generator(nn.Module):
    def __init__(self, z_dim=100, img_dim=784):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(z_dim + 10, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, img_dim),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        x = torch.cat([noise, labels], dim=1)
        return self.model(x)

def label_to_onehot(label, num_classes=10):
    return torch.eye(num_classes)[label]

z_dim = 100
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
G = Generator().to(device)
G.load_state_dict(torch.load("mnist_generator.pth", map_location=device))
G.eval()

st.title("MNIST Digit Generator")
digit = st.selectbox("Select a digit (0-9)", list(range(10)))
generate = st.button("Generate")

if generate:
    fig, axs = plt.subplots(1, 5, figsize=(15, 3))
    for i in range(5):
        noise = torch.randn(1, z_dim).to(device)
        label = label_to_onehot(torch.tensor([digit]), 10).to(device)
        with torch.no_grad():
            fake_img = G(noise, label).cpu().view(28, 28)
        axs[i].imshow(fake_img, cmap='gray')
        axs[i].axis('off')
    st.pyplot(fig)